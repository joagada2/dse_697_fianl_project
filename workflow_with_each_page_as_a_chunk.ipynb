{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c819c7-b930-4bb7-b322-f9d1b21e8b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import hashlib\n",
    "import json\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import (\n",
    "    UnstructuredPDFLoader, UnstructuredWordDocumentLoader\n",
    ")\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# ---------- Load env variables -----------\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\", \"us-east-1\")\n",
    "INDEX_NAME = \"ut-rag-app-2\"\n",
    "\n",
    "BATCH_SIZE = int(os.getenv(\"PINECONE_BATCH_SIZE\", 100))\n",
    "MAX_VECTOR_PAYLOAD_BYTES = 4 * 1024 * 1024  # 4MB\n",
    "EMBED_BATCH_SIZE = 50  # max chunk size for embedding\n",
    "\n",
    "AWS_REGION = \"us-east-2\"  # region from your ARN\n",
    "DYNAMO_TABLE_NAME = \"ScrapedPages\"  # from your ARN\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------- Pinecone init -----------\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "if INDEX_NAME not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=1536,  # Must match your embedding dimension\n",
    "        spec=ServerlessSpec(cloud='aws', region=PINECONE_API_ENV)\n",
    "    )\n",
    "pinecone_index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# ---------- Embeddings init -----------\n",
    "# Use OllamaEmbeddings or OpenAIEmbeddings, but be sure dimension matches Pinecone\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ---------- DynamoDB init -----------\n",
    "dynamo = boto3.resource(\n",
    "    \"dynamodb\",\n",
    "    region_name=AWS_REGION,\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    ")\n",
    "table = dynamo.Table(DYNAMO_TABLE_NAME)\n",
    "\n",
    "########################################\n",
    "# Comprehensive Text Preprocessing\n",
    "########################################\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    A comprehensive text preprocessing pipeline.\n",
    "    1) Lowercase\n",
    "    2) Normalize unicode\n",
    "    3) Remove HTML tags\n",
    "    4) Remove punctuation\n",
    "    5) Collapse extra whitespace\n",
    "    6) (Optional) remove stopwords if desired\n",
    "    \"\"\"\n",
    "    # 1) Lowercase\n",
    "    #text = text.lower()\n",
    "    # 2) Normalize unicode\n",
    "    #text = unicodedata.normalize(\"NFKD\", text)\n",
    "    # 3) Remove HTML tags\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    # 4) Remove punctuation\n",
    "    #text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # 5) Collapse extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # 6) (Optional) remove stopwords\n",
    "    # e.g. with nltk stopwords, user must install nltk + download\n",
    "    # from nltk.corpus import stopwords\n",
    "    # tokens = text.split()\n",
    "    # tokens = [t for t in tokens if t not in stopwords.words(\"english\")]\n",
    "    # text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "def fetch_items_from_dynamodb() -> list:\n",
    "    logger.info(\"Fetching items from DynamoDB table: %s\", DYNAMO_TABLE_NAME)\n",
    "    items = []\n",
    "    response = table.scan()\n",
    "    items.extend(response.get(\"Items\", []))\n",
    "\n",
    "    # Handle pagination\n",
    "    while 'LastEvaluatedKey' in response:\n",
    "        response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "        items.extend(response.get(\"Items\", []))\n",
    "\n",
    "    logger.info(f\"Fetched {len(items)} items from DynamoDB.\")\n",
    "    return items\n",
    "\n",
    "\n",
    "def build_documents_from_items(items: list) -> list:\n",
    "    \"\"\"\n",
    "    Convert each item into a Document.\n",
    "    The text is in 'scraped_text'.\n",
    "    The 'url' is stored in metadata.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for item in items:\n",
    "        text = item.get(\"scraped_text\")\n",
    "        url = item.get(\"url\") or \"N/A\"\n",
    "\n",
    "        if not text or not isinstance(text, str):\n",
    "            logger.warning(\"Skipping item with no or non-string 'scraped_text': %s\", item)\n",
    "            continue\n",
    "\n",
    "        # -------------- Preprocess here --------------\n",
    "        text = preprocess_text(text)\n",
    "        # --------------------------------------------\n",
    "\n",
    "        doc = Document(page_content=text, metadata={\"url\": url})\n",
    "        docs.append(doc)\n",
    "\n",
    "    logger.info(f\"Built {len(docs)} Documents from Dynamo items.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "def split_documents(docs: list) -> list:\n",
    "    \"\"\"\n",
    "    Treat each document (each page from DynamoDB) as its own chunk.\n",
    "    \"\"\"\n",
    "    return docs\n",
    "\n",
    "\n",
    "def estimate_payload_size(vector: dict) -> int:\n",
    "    return len(json.dumps(vector).encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def embed_in_batches(texts, batch_size=50):\n",
    "    all_embeds = []\n",
    "    total = len(texts)\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        start = time.time()\n",
    "        logger.info(f\"Embedding batch {i} to {i + len(batch)} / {total} ...\")\n",
    "        batch_embeds = embeddings.embed_documents(batch)\n",
    "        elapsed = time.time() - start\n",
    "        logger.info(f\"Batch embed took {elapsed:.2f} seconds\")\n",
    "        all_embeds.extend(batch_embeds)\n",
    "    return all_embeds\n",
    "\n",
    "\n",
    "def index_documents(chunks: list, batch_size: int = BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Embeds each chunk in smaller EMBED_BATCH_SIZE groups + upserts into Pinecone in safe-size batches.\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        logger.warning(\"No chunks provided for indexing.\")\n",
    "        return\n",
    "\n",
    "    # 1) Batch embed\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    embedded_vectors = embed_in_batches(texts, EMBED_BATCH_SIZE)\n",
    "\n",
    "    # 2) Create upsert payload\n",
    "    vectors = []\n",
    "    for chunk, vector in zip(chunks, embedded_vectors):\n",
    "        text = chunk.page_content\n",
    "        vector_id = hashlib.sha256(text.encode()).hexdigest()\n",
    "        meta = {\n",
    "            \"text\": text,\n",
    "            \"url\": chunk.metadata[\"url\"]\n",
    "        }\n",
    "        vectors.append({\n",
    "            \"id\": vector_id,\n",
    "            \"values\": vector,\n",
    "            \"metadata\": meta\n",
    "        })\n",
    "\n",
    "    if not vectors:\n",
    "        logger.warning(\"No vectors to upsert.\")\n",
    "        return\n",
    "\n",
    "    # 3) Upsert in safe-size batches\n",
    "    logger.info(f\"Upserting {len(vectors)} vectors to Pinecone in batches of {batch_size}...\")\n",
    "    batch = []\n",
    "    current_size = 0\n",
    "\n",
    "    for vec in tqdm(vectors, desc=\"Indexing chunks\"):\n",
    "        est_size = estimate_payload_size(vec)\n",
    "        if current_size + est_size > MAX_VECTOR_PAYLOAD_BYTES or len(batch) >= batch_size:\n",
    "            pinecone_index.upsert(vectors=batch)\n",
    "            batch = []\n",
    "            current_size = 0\n",
    "        batch.append(vec)\n",
    "        current_size += est_size\n",
    "\n",
    "    # final partial batch\n",
    "    if batch:\n",
    "        pinecone_index.upsert(vectors=batch)\n",
    "\n",
    "    logger.info(\"Indexing complete.\")\n",
    "\n",
    "\n",
    "def rag_workflow_dynamodb():\n",
    "    \"\"\"Fully self-contained RAG ingestion using DynamoDB -> Pinecone with batching + text preprocessing.\"\"\"\n",
    "    items = fetch_items_from_dynamodb()\n",
    "    if not items:\n",
    "        logger.warning(\"No items found in DynamoDB.\")\n",
    "        return\n",
    "\n",
    "    docs = build_documents_from_items(items)\n",
    "    if not docs:\n",
    "        logger.warning(\"No valid docs to index.\")\n",
    "        return\n",
    "\n",
    "    chunks = split_documents(docs)\n",
    "    logger.info(f\"Created {len(chunks)} chunks from documents.\")\n",
    "    index_documents(chunks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Just call the ingestion function\n",
    "    rag_workflow_dynamodb()\n",
    "\n",
    "\n",
    "\n",
    "# 1) Compute chunk sizes in characters\n",
    "lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "# 2) Plot the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lengths, bins=50)\n",
    "plt.title(\"Chunk Size Distribution\")\n",
    "plt.xlabel(\"Chunk length (characters)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Visualize embedded vector\n",
    "embedded_vectors = embed_in_batches([chunk.page_content for chunk in chunks])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Running in collab\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# coords is your PCA output (N×3 array)\n",
    "df = pd.DataFrame(coords, columns=[\"PC1\",\"PC2\",\"PC3\"])\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df, x=\"PC1\", y=\"PC2\", z=\"PC3\",\n",
    "    title=\"3D PCA Projection of Document Embeddings\"\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
